# Audio Synthesis

The goal is to develop an audio synthesis engine that
accepts musical notes and other metadata from 
the main JavaScript thread.

Certain aspects of the generated music will be driven by an existing
system that runs 3D reaction-diffusion style simulations in wasm
and hand-generated meshes to the main web thread where they're
sent to WebGL.

- The low-level engine is Supersonic (https://github.com/samaaron/supersonic) 
  that runs in an AudioWorklet.
- main.js contains functions for sending musical notes to synths.
- SuperCollider SynthDefs can be loaded into Supersonic
- The compute worker exports some statistics about the scalar field
  immediately around the camera to the main thread.

## Data flow

* `simulation(wasm)` -> `scalar lattice` -> `compute_worker.js` 
* `compute_worker.js` -> `camera neighbourhood stats` -> `main.js` 
* `main.js` -> `smoothed stats` -> notes generated using `music.js` -> call functions from `synth.js` 
* `synth.js` calls Supersonic 

## Synth engine

Supersonic already runs in an AudioWorklet.
Initially, we'll call it from main.js
using a helper library.

Supersonic API overview:
- `await supersonic.init()` must be called from a user interaction handler due to autoplay policy.
- `await supersonic.loadSynthDef(nameOrPath)` (or `loadSynthDefs([...])`) loads SynthDefs.
- `await supersonic.sync()` can be used to ensure the server has processed all pending commands (safe point before first notes).
- `supersonic.send(address, ...args)` sends OSC messages to scsynth (e.g. `/s_new`, `/n_set`, `/n_free`).
- Use the `setup` event to (re)create persistent groups/FX chains after `init()` and after recovery.
- Transport `mode` should default to `postMessage`. Consider `sab` later if/when we can set COOP/COEP headers.

Functions in new helper library. e.g. synth.js
- load SynthDefs 
- send commands to Supersonic
  - note/frequency
  - attack
  - delay
  - sustain
  - release
  - gain
  - pan
  - reverb
  - etc.
- It should be possible to send chords (multiple notes)
  to a synth

Implementation sketch for `synth.js`:
- Own a single `SuperSonic` instance.
- Expose `boot()` that calls `init()` and loads required SynthDefs.
- Expose `noteOn(params)` implemented via `send('/s_new', synthName, -1, addAction, targetGroup, ...kvPairs)`.
- Expose `set(nodeId, params)` implemented via `send('/n_set', nodeId, ...kvPairs)`.
- Expose `free(nodeId)` implemented via `send('/n_free', nodeId)`.

## Music composition

A helper library called music.js that contains 
- a basic finite state machine (or similar structure) 
  that outputs musical chords functioning as a potentially 
  stochastic chord progression.
  (i.e. transitions branch randomly)
  - This should be extensible to new progressions/generative
    compositions.
- A system for taking chords and arpeggiating chords into
  individual notes that can be fed into the synth engine
- Chords can be simplified, but should map onto Western music
  theory to avoid re-inventing the basics

## Camera neighbourhood voxel stats

Along with geometry and general stats, the compute worker
should periodically send some basic statistics about the 
scalar field around the camera. This could be a 16^3 cell
neighbourhood to start with.

The goal is for the statistics to give an approximation
of the state of the simulation proximal to the camera.
These stats should include
- Mean value
- Median value
- Max value
- Min value
- Range (max-min)


The stats will be used to drive musical parameters,
for things like filter frequencies, reverb room sizes
and amounts etc. As such they MUST be made available
in main.js as smoothly interpolated between published
simulation steps.

Assume scalar stats are normalized (0..1). All mappings to synth parameters MUST clamp.

## Algorithmic music MVP

The definition of done MUST be a simple subtractive
SynthDef that plays notes generated by music.js. 
The notes are arpeggiated based on the Major 7th
or similar ambient music progressions encoded
in a state machine.

The main thread SHOULD emit notes based on a hard-coded
BPM (start with 100). Notes CAN be scheduled 
somewhat ahead of time.

The synth MUST use a low-pass filter; cutoff
frequency SHOULD be determined 
by the smoothed average of the camera's voxel
neighbourhood.

The synth MUST apply a final reverb UGen to
the voice, and the room size of the reverb
should be based on the inverse of the smoothed
maximum of the camera's voxel neighbourhood. 
